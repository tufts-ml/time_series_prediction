{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# sys.path.append('./GRU-D/')\n",
    "# from models import create_grud_model\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "PROJECT_SRC_DIR = '/cluster/tufts/hugheslab/prath01/projects/time_series_prediction/src/'\n",
    "sys.path.append(PROJECT_SRC_DIR)\n",
    "sys.path.append(os.path.join(PROJECT_SRC_DIR, \"rnn\"))\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "from dataset_loader import TidySequentialDataCSVLoader\n",
    "from feature_transformation import parse_id_cols, parse_output_cols, parse_feature_cols, parse_id_cols, parse_time_cols\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load eicu data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_dir = '/cluster/tufts/hugheslab/prath01/projects/time_series_prediction/datasets/eicu/v20210610/split-by=subject_id/features_per_timeslice/classifier_train_test_split_dir/'\n",
    "x_dict_file = '/cluster/tufts/hugheslab/prath01/projects/time_series_prediction/datasets/eicu/v20210610/split-by=subject_id/features_per_timeslice/classifier_train_test_split_dir/x_dict.json'\n",
    "y_dict_file = '/cluster/tufts/hugheslab/prath01/projects/time_series_prediction/datasets/eicu/v20210610/split-by=subject_id/features_per_timeslice/classifier_train_test_split_dir/y_dict.json'\n",
    "\n",
    "x_train_csv_filename = os.path.join(train_test_dir, 'x_train_first_24_hours.csv')\n",
    "y_train_csv_filename = os.path.join(train_test_dir, 'y_train_first_24_hours.csv')\n",
    "# x_train_df = pd.read_csv(x_train_csv_filename)\n",
    "# y_train_df = pd.read_csv(y_train_csv_filename)\n",
    "\n",
    "x_valid_csv_filename = os.path.join(train_test_dir, 'x_valid_first_24_hours.csv')\n",
    "y_valid_csv_filename = os.path.join(train_test_dir, 'y_valid_first_24_hours.csv')\n",
    "# x_valid_df = pd.read_csv(x_valid_csv_filename)\n",
    "# y_valid_df = pd.read_csv(y_valid_csv_filename)\n",
    "\n",
    "x_test_csv_filename = os.path.join(train_test_dir, 'x_test_first_24_hours.csv')\n",
    "y_test_csv_filename = os.path.join(train_test_dir, 'y_test_first_24_hours.csv')\n",
    "# x_test_df = pd.read_csv(x_test_csv_filename)\n",
    "# y_test_df = pd.read_csv(y_test_csv_filename)\n",
    "\n",
    "\n",
    "train_x = pd.read_csv(x_train_csv_filename)\n",
    "valid_x = pd.read_csv(x_valid_csv_filename)\n",
    "test_x = pd.read_csv(x_test_csv_filename)\n",
    "train_y = pd.read_csv(y_train_csv_filename)\n",
    "valid_y = pd.read_csv(y_valid_csv_filename)\n",
    "test_y = pd.read_csv(y_test_csv_filename)\n",
    "\n",
    "x_data_dict = json.load(open(x_dict_file))\n",
    "y_data_dict = json.load(open(y_dict_file))\n",
    "\n",
    "feature_cols = parse_feature_cols(x_data_dict['schema'])\n",
    "id_cols = parse_id_cols(x_data_dict['schema'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>icustay_id</th>\n",
       "      <th>hours_in</th>\n",
       "      <th>BUN</th>\n",
       "      <th>Hct</th>\n",
       "      <th>age</th>\n",
       "      <th>bedside glucose</th>\n",
       "      <th>creatinine</th>\n",
       "      <th>gender_is_male</th>\n",
       "      <th>gender_is_unknown</th>\n",
       "      <th>glucose</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>noninvasive_diastolic</th>\n",
       "      <th>noninvasive_systolic</th>\n",
       "      <th>potassium</th>\n",
       "      <th>respiratory_rate</th>\n",
       "      <th>sao2</th>\n",
       "      <th>st1</th>\n",
       "      <th>st2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>002-10052</td>\n",
       "      <td>137239</td>\n",
       "      <td>151900</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>48.500000</td>\n",
       "      <td>73.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>002-10052</td>\n",
       "      <td>137239</td>\n",
       "      <td>151900</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>50.500000</td>\n",
       "      <td>86.375000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>002-10052</td>\n",
       "      <td>137239</td>\n",
       "      <td>151900</td>\n",
       "      <td>2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>52.400000</td>\n",
       "      <td>95.400000</td>\n",
       "      <td>3.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>002-10052</td>\n",
       "      <td>137239</td>\n",
       "      <td>151900</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>73.166664</td>\n",
       "      <td>108.333336</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>002-10052</td>\n",
       "      <td>137239</td>\n",
       "      <td>151900</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.800000</td>\n",
       "      <td>67.200000</td>\n",
       "      <td>114.400000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>002-10052</td>\n",
       "      <td>137239</td>\n",
       "      <td>151900</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.666664</td>\n",
       "      <td>58.200000</td>\n",
       "      <td>105.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>002-10052</td>\n",
       "      <td>137239</td>\n",
       "      <td>151900</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.3</td>\n",
       "      <td>66.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>52.428570</td>\n",
       "      <td>103.285710</td>\n",
       "      <td>4.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>002-10052</td>\n",
       "      <td>137239</td>\n",
       "      <td>151900</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.666664</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>111.666664</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>002-10052</td>\n",
       "      <td>137239</td>\n",
       "      <td>151900</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.400000</td>\n",
       "      <td>64.800000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>002-10052</td>\n",
       "      <td>137239</td>\n",
       "      <td>151900</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.333336</td>\n",
       "      <td>61.333332</td>\n",
       "      <td>113.333336</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>002-10052</td>\n",
       "      <td>137239</td>\n",
       "      <td>151900</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>51.600000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>002-10052</td>\n",
       "      <td>137239</td>\n",
       "      <td>151900</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>89.333336</td>\n",
       "      <td>53.250000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>002-10052</td>\n",
       "      <td>137239</td>\n",
       "      <td>151900</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.400000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>002-10052</td>\n",
       "      <td>137239</td>\n",
       "      <td>151900</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.666664</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>002-10052</td>\n",
       "      <td>137239</td>\n",
       "      <td>151900</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.800000</td>\n",
       "      <td>58.200000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>002-10052</td>\n",
       "      <td>137239</td>\n",
       "      <td>151900</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.1</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>89.333336</td>\n",
       "      <td>61.666668</td>\n",
       "      <td>106.666664</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>002-10052</td>\n",
       "      <td>137239</td>\n",
       "      <td>151900</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95.200000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>108.400000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95.600000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>002-10052</td>\n",
       "      <td>137239</td>\n",
       "      <td>151900</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>58.333332</td>\n",
       "      <td>104.333336</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>002-10052</td>\n",
       "      <td>137239</td>\n",
       "      <td>151900</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.400000</td>\n",
       "      <td>59.400000</td>\n",
       "      <td>111.800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.333334</td>\n",
       "      <td>95.400000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>002-10052</td>\n",
       "      <td>137239</td>\n",
       "      <td>151900</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>89.333336</td>\n",
       "      <td>58.333332</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>002-10052</td>\n",
       "      <td>137239</td>\n",
       "      <td>151900</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90.400000</td>\n",
       "      <td>54.400000</td>\n",
       "      <td>101.400000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.600000</td>\n",
       "      <td>94.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>002-10052</td>\n",
       "      <td>137239</td>\n",
       "      <td>151900</td>\n",
       "      <td>21</td>\n",
       "      <td>15.0</td>\n",
       "      <td>22.4</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>93.333336</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>98.666664</td>\n",
       "      <td>3.4</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>93.666664</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>002-10052</td>\n",
       "      <td>137239</td>\n",
       "      <td>151900</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92.400000</td>\n",
       "      <td>52.200000</td>\n",
       "      <td>95.400000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>93.800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>002-10052</td>\n",
       "      <td>137239</td>\n",
       "      <td>151900</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>97.333336</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>002-10052</td>\n",
       "      <td>137239</td>\n",
       "      <td>151900</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>112.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>002-10063</td>\n",
       "      <td>189145</td>\n",
       "      <td>218742</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id  hadm_id  icustay_id  hours_in   BUN   Hct   age  \\\n",
       "0   002-10052   137239      151900         0   NaN   NaN  66.0   \n",
       "1   002-10052   137239      151900         1   NaN   NaN  66.0   \n",
       "2   002-10052   137239      151900         2  19.0   NaN  66.0   \n",
       "3   002-10052   137239      151900         3   NaN   NaN  66.0   \n",
       "4   002-10052   137239      151900         4   NaN   NaN  66.0   \n",
       "5   002-10052   137239      151900         5   NaN   NaN  66.0   \n",
       "6   002-10052   137239      151900         6   NaN  24.3  66.0   \n",
       "7   002-10052   137239      151900         7   NaN   NaN  66.0   \n",
       "8   002-10052   137239      151900         8   NaN   NaN  66.0   \n",
       "9   002-10052   137239      151900         9   NaN   NaN  66.0   \n",
       "10  002-10052   137239      151900        10   NaN   NaN  66.0   \n",
       "11  002-10052   137239      151900        11   NaN   NaN  66.0   \n",
       "12  002-10052   137239      151900        12   NaN   NaN  66.0   \n",
       "13  002-10052   137239      151900        13   NaN   NaN  66.0   \n",
       "14  002-10052   137239      151900        14   NaN   NaN  66.0   \n",
       "15  002-10052   137239      151900        15   NaN  23.1  66.0   \n",
       "16  002-10052   137239      151900        16   NaN   NaN  66.0   \n",
       "17  002-10052   137239      151900        17   NaN   NaN  66.0   \n",
       "18  002-10052   137239      151900        18   NaN   NaN  66.0   \n",
       "19  002-10052   137239      151900        19   NaN   NaN  66.0   \n",
       "20  002-10052   137239      151900        20   NaN   NaN  66.0   \n",
       "21  002-10052   137239      151900        21  15.0  22.4  66.0   \n",
       "22  002-10052   137239      151900        22   NaN   NaN  66.0   \n",
       "23  002-10052   137239      151900        23   NaN   NaN  66.0   \n",
       "24  002-10052   137239      151900        24   NaN   NaN  66.0   \n",
       "25  002-10063   189145      218742         0   NaN   NaN  69.0   \n",
       "\n",
       "    bedside glucose  creatinine  gender_is_male  gender_is_unknown  glucose  \\\n",
       "0               NaN         NaN             0.0                0.0      NaN   \n",
       "1               NaN         NaN             0.0                0.0      NaN   \n",
       "2               NaN        0.84             0.0                0.0    139.0   \n",
       "3               NaN         NaN             0.0                0.0      NaN   \n",
       "4               NaN         NaN             0.0                0.0      NaN   \n",
       "5               NaN         NaN             0.0                0.0      NaN   \n",
       "6             116.0         NaN             0.0                0.0      NaN   \n",
       "7               NaN         NaN             0.0                0.0      NaN   \n",
       "8               NaN         NaN             0.0                0.0      NaN   \n",
       "9               NaN         NaN             0.0                0.0      NaN   \n",
       "10              NaN         NaN             0.0                0.0      NaN   \n",
       "11              NaN         NaN             0.0                0.0      NaN   \n",
       "12            113.0         NaN             0.0                0.0      NaN   \n",
       "13              NaN         NaN             0.0                0.0      NaN   \n",
       "14              NaN         NaN             0.0                0.0      NaN   \n",
       "15              NaN         NaN             0.0                0.0      NaN   \n",
       "16              NaN         NaN             0.0                0.0      NaN   \n",
       "17              NaN         NaN             0.0                0.0      NaN   \n",
       "18             90.0         NaN             0.0                0.0      NaN   \n",
       "19              NaN         NaN             0.0                0.0      NaN   \n",
       "20              NaN         NaN             0.0                0.0      NaN   \n",
       "21              NaN        0.85             0.0                0.0     94.0   \n",
       "22              NaN         NaN             0.0                0.0      NaN   \n",
       "23              NaN         NaN             0.0                0.0      NaN   \n",
       "24            101.0         NaN             0.0                0.0      NaN   \n",
       "25              NaN         NaN             0.0                0.0      NaN   \n",
       "\n",
       "    heart_rate  noninvasive_diastolic  noninvasive_systolic  potassium  \\\n",
       "0    90.000000              48.500000             73.500000        NaN   \n",
       "1    74.000000              50.500000             86.375000        NaN   \n",
       "2    78.000000              52.400000             95.400000        3.7   \n",
       "3    77.000000              73.166664            108.333336        NaN   \n",
       "4    80.800000              67.200000            114.400000        NaN   \n",
       "5    72.666664              58.200000            105.200000        NaN   \n",
       "6    74.000000              52.428570            103.285710        4.2   \n",
       "7    72.666664              55.000000            111.666664        NaN   \n",
       "8    84.400000              64.800000            125.000000        NaN   \n",
       "9    83.333336              61.333332            113.333336        NaN   \n",
       "10   92.000000              51.600000            100.000000        NaN   \n",
       "11   89.333336              53.250000             98.000000        NaN   \n",
       "12   86.400000              59.000000            107.000000        NaN   \n",
       "13   86.000000              61.000000            109.000000        NaN   \n",
       "14   84.800000              58.200000            109.000000        NaN   \n",
       "15   89.333336              61.666668            106.666664        NaN   \n",
       "16   95.200000              64.000000            108.400000        NaN   \n",
       "17   84.000000              58.333332            104.333336        NaN   \n",
       "18   86.400000              59.400000            111.800000        NaN   \n",
       "19   89.333336              58.333332            109.000000        NaN   \n",
       "20   90.400000              54.400000            101.400000        NaN   \n",
       "21   93.333336              50.000000             98.666664        3.4   \n",
       "22   92.400000              52.200000             95.400000        NaN   \n",
       "23   90.000000              49.000000             97.333336        NaN   \n",
       "24   92.000000              65.000000            112.500000        NaN   \n",
       "25         NaN                    NaN                   NaN        NaN   \n",
       "\n",
       "    respiratory_rate        sao2  st1  st2  \n",
       "0                NaN  100.000000  NaN  NaN  \n",
       "1                NaN  100.000000  NaN  NaN  \n",
       "2                NaN  100.000000  NaN  NaN  \n",
       "3                NaN  100.000000  NaN  NaN  \n",
       "4                NaN   99.800000  NaN  NaN  \n",
       "5                NaN  100.000000  NaN  NaN  \n",
       "6                NaN  100.000000  NaN  NaN  \n",
       "7                NaN  100.000000  NaN  NaN  \n",
       "8                NaN   99.200000  NaN  NaN  \n",
       "9                NaN   99.000000  NaN  NaN  \n",
       "10               NaN   99.000000  NaN  NaN  \n",
       "11               NaN  100.000000  NaN  NaN  \n",
       "12               NaN   99.800000  NaN  NaN  \n",
       "13               NaN   99.666664  NaN  NaN  \n",
       "14               NaN   99.800000  NaN  NaN  \n",
       "15               NaN   96.000000  NaN  NaN  \n",
       "16               NaN   95.600000  NaN  NaN  \n",
       "17               NaN   98.000000  NaN  NaN  \n",
       "18         17.333334   95.400000  NaN  NaN  \n",
       "19         15.000000   92.000000  NaN  NaN  \n",
       "20         14.600000   94.200000  NaN  NaN  \n",
       "21         18.000000   93.666664  NaN  NaN  \n",
       "22         16.000000   93.800000  NaN  NaN  \n",
       "23         15.000000   95.000000  NaN  NaN  \n",
       "24         15.000000   96.000000  NaN  NaN  \n",
       "25               NaN         NaN  NaN  NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.iloc[:26, :20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vitals = TidySequentialDataCSVLoader(\n",
    "    x_csv_path=train_x,\n",
    "    y_csv_path=train_y,\n",
    "    x_col_names=feature_cols,\n",
    "    idx_col_names=id_cols,\n",
    "    y_col_name=\"mort_hosp\",\n",
    "    y_label_type='per_sequence'\n",
    ")\n",
    "\n",
    "valid_vitals = TidySequentialDataCSVLoader(\n",
    "    x_csv_path=valid_x,\n",
    "    y_csv_path=valid_y,\n",
    "    x_col_names=feature_cols,\n",
    "    idx_col_names=id_cols,\n",
    "    y_col_name=\"mort_hosp\",\n",
    "    y_label_type='per_sequence'\n",
    ")\n",
    "\n",
    "test_vitals = TidySequentialDataCSVLoader(\n",
    "    x_csv_path=test_x,\n",
    "    y_csv_path=test_y,\n",
    "    x_col_names=feature_cols,\n",
    "    idx_col_names=id_cols,\n",
    "    y_col_name=\"mort_hosp\",\n",
    "    y_label_type='per_sequence'\n",
    ")\n",
    "\n",
    "# num_true_feats = int(F/3)\n",
    "train_x_NTD, y_train = train_vitals.get_batch_data(batch_id=0)\n",
    "valid_x_NTD, y_valid = valid_vitals.get_batch_data(batch_id=0)\n",
    "test_x_NTD, y_test = test_vitals.get_batch_data(batch_id=0)\n",
    "\n",
    "N_tr = len(train_x_NTD)\n",
    "N_va = len(valid_x_NTD)\n",
    "N_te = len(test_x_NTD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction positive labels in train set : 0.0825\n",
      "fraction positive labels in valid set : 0.0814\n",
      "fraction positive labels in test set : 0.0831\n"
     ]
    }
   ],
   "source": [
    "for split, y in [('train', y_train),\n",
    "                ('valid', y_valid),\n",
    "                ('test', y_test)]:\n",
    "    print('fraction positive labels in %s set : %.4f'%(split, y.sum()/len(y)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "CREATING TRAIN/VALID/TEST SPLITS FOR 1.200 PERCENT OF SEQUENCES LABELLED\n",
      "---------------------------------------------------------------------------\n",
      "Excluded inds train: 15662, 21871, 28805 ... 29562, 34572, 21708\n",
      "Excluded inds valid: 9691, 7743, 866 ... 4256, 6648, 5532\n",
      "Excluded inds test: 12636, 14342, 224 ... 5861, 7136, 14248\n",
      "Saving data to /cluster/tufts/hugheslab/prath01/datasets/eicu_ssl/percentage_labelled_sequnces=1.2\n",
      "Done saving train..\n",
      "Done saving valid..\n",
      "Done saving test..\n",
      "---------------------------------------------------------------------------\n",
      "fraction positive labels in train set with 1.200 percent of sequences labelled : 0.0986\n",
      "fraction positive labels in valid set with 1.200 percent of sequences labelled : 0.0709\n",
      "fraction positive labels in test set with 1.200 percent of sequences labelled : 0.0400\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "state_id = 41\n",
    "data_save_dir = '/cluster/tufts/hugheslab/prath01/datasets/eicu_ssl/'\n",
    "\n",
    "for ii, perc_labelled in enumerate([1.2]):#3.7, 11.1, 33.3, 100\n",
    "    curr_save_dir = os.path.join(data_save_dir, 'percentage_labelled_sequnces=%s'%perc_labelled)\n",
    "    \n",
    "    print('---------------------------------------------------------------------------')\n",
    "    print('CREATING TRAIN/VALID/TEST SPLITS FOR %.3f PERCENT OF SEQUENCES LABELLED'%perc_labelled)\n",
    "    print('---------------------------------------------------------------------------')\n",
    "    y_train_ss = y_train.copy()\n",
    "    rnd_state = np.random.RandomState(state_id)\n",
    "    n_unlabelled_tr = int((1-(perc_labelled)/100)*N_tr)\n",
    "    unlabelled_inds_tr = rnd_state.permutation(N_tr)[:n_unlabelled_tr]\n",
    "    y_train_ss = y_train_ss.astype(np.float32)\n",
    "    y_train_ss[unlabelled_inds_tr] = np.nan  \n",
    "    if perc_labelled!=100:\n",
    "        print('Excluded inds train: %d, %d, %d ... %d, %d, %d'%(unlabelled_inds_tr[0],\n",
    "                                                              unlabelled_inds_tr[1],\n",
    "                                                              unlabelled_inds_tr[2],\n",
    "                                                              unlabelled_inds_tr[-3],\n",
    "                                                              unlabelled_inds_tr[-2],\n",
    "                                                              unlabelled_inds_tr[-1]))\n",
    "    \n",
    "    y_valid_ss = y_valid.copy()\n",
    "    rnd_state = np.random.RandomState(state_id)\n",
    "    n_unlabelled_va = int((1-(perc_labelled)/100)*N_va)\n",
    "    unlabelled_inds_va = rnd_state.permutation(N_va)[:n_unlabelled_va]\n",
    "    y_valid_ss = y_valid_ss.astype(np.float32)\n",
    "    y_valid_ss[unlabelled_inds_va] = np.nan \n",
    "    if perc_labelled!=100:\n",
    "        print('Excluded inds valid: %d, %d, %d ... %d, %d, %d'%(unlabelled_inds_va[0],\n",
    "                                                          unlabelled_inds_va[1],\n",
    "                                                          unlabelled_inds_va[2],\n",
    "                                                          unlabelled_inds_va[-3],\n",
    "                                                          unlabelled_inds_va[-2],\n",
    "                                                          unlabelled_inds_va[-1]))\n",
    "\n",
    "    y_test_ss = y_test.copy()\n",
    "    rnd_state = np.random.RandomState(state_id)\n",
    "    n_unlabelled_te = int((1-(perc_labelled)/100)*N_te)\n",
    "    unlabelled_inds_te = rnd_state.permutation(N_te)[:n_unlabelled_te]\n",
    "    y_test_ss = y_test_ss.astype(np.float32)\n",
    "    y_test_ss[unlabelled_inds_te] = np.nan\n",
    "    if perc_labelled!=100:\n",
    "        print('Excluded inds test: %d, %d, %d ... %d, %d, %d'%(unlabelled_inds_te[0],\n",
    "                                                          unlabelled_inds_te[1],\n",
    "                                                          unlabelled_inds_te[2],\n",
    "                                                          unlabelled_inds_te[-3],\n",
    "                                                          unlabelled_inds_te[-2],\n",
    "                                                          unlabelled_inds_te[-1]))\n",
    "    \n",
    "    # Check whether the specified path exists or not\n",
    "    isExist = os.path.exists(curr_save_dir)\n",
    "\n",
    "    if not isExist:\n",
    "        # Create a new directory because it does not exist \n",
    "        os.makedirs(curr_save_dir)\n",
    "        \n",
    "    # save the data to the respective folder\n",
    "    print('Saving data to %s'%curr_save_dir)\n",
    "    np.save(os.path.join(curr_save_dir, 'X_train.npy'), train_x_NTD)\n",
    "    np.save(os.path.join(curr_save_dir, 'y_train.npy'), y_train_ss)\n",
    "    print('Done saving train..')\n",
    "    np.save(os.path.join(curr_save_dir, 'X_valid.npy'), valid_x_NTD)\n",
    "    np.save(os.path.join(curr_save_dir, 'y_valid.npy'), y_valid_ss)\n",
    "    print('Done saving valid..')\n",
    "    np.save(os.path.join(curr_save_dir, 'X_test.npy'), test_x_NTD)\n",
    "    np.save(os.path.join(curr_save_dir, 'y_test.npy'), y_test_ss)\n",
    "    print('Done saving test..')\n",
    "    \n",
    "    \n",
    "    print('---------------------------------------------------------------------------')\n",
    "    for split, y in [('train', y_train_ss),\n",
    "                    ('valid', y_valid_ss),\n",
    "                    ('test', y_test_ss)]:\n",
    "        frac_pos_labels = np.nansum(y)/(~np.isnan(y)).sum()\n",
    "        print('fraction positive labels in %s set with %.3f percent of sequences labelled : %.4f'%(split,\n",
    "                                                                                                   perc_labelled,\n",
    "                                                                                                   frac_pos_labels))\n",
    "    print('---------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "fraction positive labels in train set with 100.000 percent of sequences labelled : 0.0825\n",
      "fraction positive labels in valid set with 100.000 percent of sequences labelled : 0.0814\n",
      "fraction positive labels in test set with 100.000 percent of sequences labelled : 0.0831\n"
     ]
    }
   ],
   "source": [
    "    print('---------------------------------------------------------------------------')\n",
    "    for split, y in [('train', y_train_ss),\n",
    "                    ('valid', y_valid_ss),\n",
    "                    ('test', y_test_ss)]:\n",
    "        frac_pos_labels = np.nansum(y)/(~np.isnan(y)).sum()\n",
    "        print('fraction positive labels in %s set with %.3f percent of sequences labelled : %.4f'%(split,\n",
    "                                                                                                   perc_labelled,\n",
    "                                                                                                   frac_pos_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid_NTD_loaded = np.load('X_valid.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 50.84156,  45.     ,   4.9    , ...,       nan,  10.4    ,\n",
       "               nan],\n",
       "        [ 50.84156,       nan,       nan, ...,       nan,       nan,\n",
       "               nan],\n",
       "        [ 50.84156,       nan,       nan, ...,       nan,       nan,\n",
       "               nan],\n",
       "        ...,\n",
       "        [ 50.84156,       nan,       nan, ...,       nan,       nan,\n",
       "               nan],\n",
       "        [ 50.84156,       nan,       nan, ...,       nan,       nan,\n",
       "               nan],\n",
       "        [ 50.84156,       nan,       nan, ...,       nan,       nan,\n",
       "               nan]],\n",
       "\n",
       "       [[300.00308,       nan,       nan, ...,       nan,       nan,\n",
       "               nan],\n",
       "        [300.00308,       nan,       nan, ...,       nan,       nan,\n",
       "               nan],\n",
       "        [300.00308,       nan,       nan, ...,       nan,       nan,\n",
       "               nan],\n",
       "        ...,\n",
       "        [300.00308,       nan,       nan, ...,       nan,       nan,\n",
       "               nan],\n",
       "        [300.00308,       nan,       nan, ...,       nan,       nan,\n",
       "               nan],\n",
       "        [300.00308,       nan,       nan, ...,       nan,       nan,\n",
       "               nan]],\n",
       "\n",
       "       [[ 69.44273,       nan,       nan, ...,       nan,       nan,\n",
       "               nan],\n",
       "        [ 69.44273,       nan,       nan, ...,       nan,       nan,\n",
       "               nan],\n",
       "        [ 69.44273,       nan,       nan, ...,       nan,  11.5    ,\n",
       "               nan],\n",
       "        ...,\n",
       "        [ 69.44273,       nan,       nan, ...,       nan,       nan,\n",
       "               nan],\n",
       "        [ 69.44273,       nan,       nan, ...,       nan,       nan,\n",
       "               nan],\n",
       "        [ 69.44273,       nan,       nan, ...,       nan,       nan,\n",
       "               nan]]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid_NTD_loaded[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 50.84156,  45.     ,   4.9    , ...,       nan,  10.4    ,\n",
       "               nan],\n",
       "        [ 50.84156,       nan,       nan, ...,       nan,       nan,\n",
       "               nan],\n",
       "        [ 50.84156,       nan,       nan, ...,       nan,       nan,\n",
       "               nan],\n",
       "        ...,\n",
       "        [ 50.84156,       nan,       nan, ...,       nan,       nan,\n",
       "               nan],\n",
       "        [ 50.84156,       nan,       nan, ...,       nan,       nan,\n",
       "               nan],\n",
       "        [ 50.84156,       nan,       nan, ...,       nan,       nan,\n",
       "               nan]],\n",
       "\n",
       "       [[300.00308,       nan,       nan, ...,       nan,       nan,\n",
       "               nan],\n",
       "        [300.00308,       nan,       nan, ...,       nan,       nan,\n",
       "               nan],\n",
       "        [300.00308,       nan,       nan, ...,       nan,       nan,\n",
       "               nan],\n",
       "        ...,\n",
       "        [300.00308,       nan,       nan, ...,       nan,       nan,\n",
       "               nan],\n",
       "        [300.00308,       nan,       nan, ...,       nan,       nan,\n",
       "               nan],\n",
       "        [300.00308,       nan,       nan, ...,       nan,       nan,\n",
       "               nan]],\n",
       "\n",
       "       [[ 69.44273,       nan,       nan, ...,       nan,       nan,\n",
       "               nan],\n",
       "        [ 69.44273,       nan,       nan, ...,       nan,       nan,\n",
       "               nan],\n",
       "        [ 69.44273,       nan,       nan, ...,       nan,  11.5    ,\n",
       "               nan],\n",
       "        ...,\n",
       "        [ 69.44273,       nan,       nan, ...,       nan,       nan,\n",
       "               nan],\n",
       "        [ 69.44273,       nan,       nan, ...,       nan,       nan,\n",
       "               nan],\n",
       "        [ 69.44273,       nan,       nan, ...,       nan,       nan,\n",
       "               nan]]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_x_NTD[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the eicu collapsed features data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from featurize_single_time_series import collapse_std, collapse_elapsed_time_since_last_measured, collapse_count, collapse_slope, collapse_median, collapse_min, collapse_max, collapse_value_last_measured, make_summary_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse features\n",
    "def featurize_ts(\n",
    "        time_arr_by_var,\n",
    "        val_arr_by_var,\n",
    "        n_features,\n",
    "        percentile_slices_to_featurize=[(0., 100.)],\n",
    "        summary_ops=['count', 'mean', 'std', 'slope'],\n",
    "        ):\n",
    "    ''' Featurize provided multivariate irregular time series into flat vector\n",
    "    Args\n",
    "    ----\n",
    "    time_arr_by_var : dict of 1D NumPy arrays\n",
    "    val_arr_by_var : dict of 1D NumPy arrays\n",
    "    start_numerictime : float\n",
    "        Indicates numerical time value at which current window *starts*\n",
    "    stop_numerictime : float\n",
    "        Indicates numerical time that current window *stops*\n",
    "    Returns\n",
    "    -------\n",
    "    feat_vec_1F : 2D NumPy array, shape (1, F)\n",
    "        One entry for each combination of {variable, summary op, subwindow slice}\n",
    "    '''\n",
    "    \n",
    "    start_numerictime = 0\n",
    "    stop_numerictime = 24\n",
    "    time_range = stop_numerictime - start_numerictime\n",
    "\n",
    "    F = len(percentile_slices_to_featurize) * n_features * len (summary_ops)\n",
    "    feat_vec_1F = np.zeros((1, F))\n",
    "    ff = 0\n",
    "\n",
    "    SUMMARY_OPERATIONS = make_summary_ops()\n",
    "\n",
    "    for rp_ind, (low, high) in enumerate(percentile_slices_to_featurize):\n",
    "        cur_window_start_time = start_numerictime + float(low) / 100 * time_range\n",
    "        cur_window_stop_time = start_numerictime + float(high) / 100 * time_range\n",
    "\n",
    "        for var_id in range(n_features):\n",
    "            cur_feat_arr = val_arr_by_var[:, var_id].astype('float')\n",
    "            cur_numerictime_arr = time_arr_by_var\n",
    "\n",
    "            # Keep only the entries whose times occur within current window\n",
    "            start = np.searchsorted(\n",
    "                cur_numerictime_arr, cur_window_start_time, side='left')\n",
    "            stop = np.searchsorted(\n",
    "                cur_numerictime_arr, cur_window_stop_time, side='right')\n",
    "            cur_numerictime_arr = cur_numerictime_arr[start:stop]\n",
    "            cur_feat_arr = cur_feat_arr[start:stop]\n",
    "            cur_isfinite_arr = np.isfinite(cur_feat_arr)\n",
    "            \n",
    "            for op_ind, op in enumerate(summary_ops):\n",
    "                summary_func, empty_val = SUMMARY_OPERATIONS[op]\n",
    "                if cur_feat_arr.size < 1 or cur_isfinite_arr.sum() < 1:\n",
    "                    feat_vec_1F[0,ff] = empty_val\n",
    "                else:\n",
    "                    feat_vec_1F[0,ff] = summary_func(\n",
    "                        cur_feat_arr, cur_numerictime_arr, cur_isfinite_arr,\n",
    "                        cur_window_start_time, cur_window_stop_time)\n",
    "#                 feat_names.append(\"feature_%s_%s_%.0f-%.0f\" % (var_id, op, float(low), float(high)))\n",
    "                ff += 1\n",
    "    return feat_vec_1F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collapsing train feaures\n",
      "Done with 0 sequences..\n",
      "Done with 500 sequences..\n",
      "Done with 1000 sequences..\n",
      "Done with 1500 sequences..\n",
      "Done with 2000 sequences..\n",
      "Done with 2500 sequences..\n",
      "Done with 3000 sequences..\n",
      "Done with 3500 sequences..\n",
      "Done with 4000 sequences..\n",
      "Done with 4500 sequences..\n",
      "Done with 5000 sequences..\n",
      "Done with 5500 sequences..\n",
      "Done with 6000 sequences..\n",
      "Done with 6500 sequences..\n",
      "Done with 7000 sequences..\n",
      "Done with 7500 sequences..\n",
      "Done with 8000 sequences..\n",
      "Done with 8500 sequences..\n",
      "Done with 9000 sequences..\n",
      "Done with 9500 sequences..\n",
      "Done with 10000 sequences..\n",
      "Done with 10500 sequences..\n",
      "Done with 11000 sequences..\n",
      "Done with 11500 sequences..\n",
      "Done with 12000 sequences..\n",
      "Done with 12500 sequences..\n",
      "Done with 13000 sequences..\n",
      "Done with 13500 sequences..\n",
      "Done with 14000 sequences..\n",
      "Done with 14500 sequences..\n",
      "Done with 15000 sequences..\n",
      "Done with 15500 sequences..\n",
      "Done with 16000 sequences..\n",
      "Done with 16500 sequences..\n",
      "Done with 17000 sequences..\n",
      "Done with 17500 sequences..\n",
      "Done with 18000 sequences..\n",
      "Done with 18500 sequences..\n",
      "Done with 19000 sequences..\n",
      "Done with 19500 sequences..\n",
      "Done with 20000 sequences..\n",
      "Done with 20500 sequences..\n",
      "Done with 21000 sequences..\n",
      "Done with 21500 sequences..\n",
      "Done with 22000 sequences..\n",
      "Done with 22500 sequences..\n",
      "Done with 23000 sequences..\n",
      "Done with 23500 sequences..\n",
      "Done with 24000 sequences..\n",
      "Done with 24500 sequences..\n",
      "Done with 25000 sequences..\n",
      "Done with 25500 sequences..\n",
      "Done with 26000 sequences..\n",
      "Done with 26500 sequences..\n",
      "Done with 27000 sequences..\n",
      "Done with 27500 sequences..\n",
      "Done with 28000 sequences..\n",
      "Done with 28500 sequences..\n",
      "Done with 29000 sequences..\n",
      "Done with 29500 sequences..\n",
      "Done with 30000 sequences..\n",
      "Done with 30500 sequences..\n",
      "Done with 31000 sequences..\n",
      "Done with 31500 sequences..\n",
      "Done with 32000 sequences..\n",
      "Done with 32500 sequences..\n",
      "Done with 33000 sequences..\n",
      "Done with 33500 sequences..\n",
      "Done with 34000 sequences..\n",
      "Done with 34500 sequences..\n",
      "Done with 35000 sequences..\n",
      "Done with 35500 sequences..\n",
      "Done with 36000 sequences..\n",
      "Done with 36500 sequences..\n",
      "Done with 37000 sequences..\n",
      "Done with 37500 sequences..\n",
      "Done with 38000 sequences..\n",
      "Done with 38500 sequences..\n",
      "Done with 39000 sequences..\n",
      "Done with 39500 sequences..\n",
      "Done with 40000 sequences..\n",
      "Done with 40500 sequences..\n",
      "Done with 41000 sequences..\n",
      "Done with 41500 sequences..\n",
      "Done with 42000 sequences..\n",
      "Done with 42500 sequences..\n",
      "Done with 43000 sequences..\n",
      "Done with 43500 sequences..\n",
      "Done with 44000 sequences..\n",
      "Done with 44500 sequences..\n",
      "Done with 45000 sequences..\n",
      "Done with 45500 sequences..\n",
      "Done with 46000 sequences..\n",
      "Collapsing valid feaures\n",
      "Done with 0 sequences..\n",
      "Done with 500 sequences..\n",
      "Done with 1000 sequences..\n",
      "Done with 1500 sequences..\n",
      "Done with 2000 sequences..\n",
      "Done with 2500 sequences..\n",
      "Done with 3000 sequences..\n",
      "Done with 3500 sequences..\n",
      "Done with 4000 sequences..\n",
      "Done with 4500 sequences..\n",
      "Done with 5000 sequences..\n",
      "Done with 5500 sequences..\n",
      "Done with 6000 sequences..\n",
      "Done with 6500 sequences..\n",
      "Done with 7000 sequences..\n",
      "Done with 7500 sequences..\n",
      "Done with 8000 sequences..\n",
      "Done with 8500 sequences..\n",
      "Done with 9000 sequences..\n",
      "Done with 9500 sequences..\n",
      "Done with 10000 sequences..\n",
      "Done with 10500 sequences..\n",
      "Done with 11000 sequences..\n",
      "Done with 11500 sequences..\n",
      "Collapsing test feaures\n",
      "Done with 0 sequences..\n",
      "Done with 500 sequences..\n",
      "Done with 1000 sequences..\n",
      "Done with 1500 sequences..\n",
      "Done with 2000 sequences..\n",
      "Done with 2500 sequences..\n",
      "Done with 3000 sequences..\n",
      "Done with 3500 sequences..\n",
      "Done with 4000 sequences..\n",
      "Done with 4500 sequences..\n",
      "Done with 5000 sequences..\n",
      "Done with 5500 sequences..\n",
      "Done with 6000 sequences..\n",
      "Done with 6500 sequences..\n",
      "Done with 7000 sequences..\n",
      "Done with 7500 sequences..\n",
      "Done with 8000 sequences..\n",
      "Done with 8500 sequences..\n",
      "Done with 9000 sequences..\n",
      "Done with 9500 sequences..\n",
      "Done with 10000 sequences..\n",
      "Done with 10500 sequences..\n",
      "Done with 11000 sequences..\n",
      "Done with 11500 sequences..\n",
      "Done with 12000 sequences..\n",
      "Done with 12500 sequences..\n",
      "Done with 13000 sequences..\n",
      "Done with 13500 sequences..\n",
      "Done with 14000 sequences..\n",
      "Done with 14500 sequences..\n"
     ]
    }
   ],
   "source": [
    "N_tr = len(train_x_NTD)\n",
    "N_va = len(valid_x_NTD)\n",
    "N_te = len(test_x_NTD)\n",
    "percentile_slices_to_featurize = [(0., 100.), (90, 100)]\n",
    "summary_ops = [\"std\", \"time_since_measured\", \"count\", \"slope\", \"median\", \"min\", \"max\", \"last_value_measured\"]\n",
    "n_features = train_x_NTD.shape[-1]\n",
    "F = len(percentile_slices_to_featurize) * n_features * len (summary_ops)\n",
    "\n",
    "\n",
    "train_x_collapsed_NF = np.zeros((N_tr, F))\n",
    "valid_x_collapsed_NF = np.zeros((N_va, F))\n",
    "test_x_collapsed_NF = np.zeros((N_te, F))\n",
    "\n",
    "print('Collapsing train feaures')\n",
    "for nn in range(N_tr):\n",
    "    if (nn%500)==0:\n",
    "        print('Done with %s sequences..'%nn)\n",
    "    train_x_collapsed_NF[nn, :] = featurize_ts(np.arange(0, 25).astype(float),\n",
    "                                               train_x_NTD[nn],\n",
    "                                               n_features,\n",
    "                                               percentile_slices_to_featurize=[(0., 100.), (90, 100)],\n",
    "                                               summary_ops=[\"std\", \"time_since_measured\", \n",
    "                                                            \"count\", \"slope\", \"median\", \n",
    "                                                            \"min\", \"max\", \"last_value_measured\"])\n",
    "print('Collapsing valid feaures')\n",
    "for nn in range(N_va):\n",
    "    if (nn%500)==0:\n",
    "        print('Done with %s sequences..'%nn)\n",
    "    valid_x_collapsed_NF[nn, :] = featurize_ts(np.arange(0, 25).astype(float),\n",
    "                                               valid_x_NTD[nn],\n",
    "                                               n_features,\n",
    "                                               percentile_slices_to_featurize=[(0., 100.), (90, 100)],\n",
    "                                               summary_ops=[\"std\", \"time_since_measured\", \n",
    "                                                            \"count\", \"slope\", \"median\", \n",
    "                                                            \"min\", \"max\", \"last_value_measured\"])\n",
    "\n",
    "print('Collapsing test feaures')\n",
    "for nn in range(N_te):\n",
    "    if (nn%500)==0:\n",
    "        print('Done with %s sequences..'%nn)\n",
    "    test_x_collapsed_NF[nn, :] = featurize_ts(np.arange(0, 25).astype(float),\n",
    "                                               test_x_NTD[nn],\n",
    "                                               n_features,\n",
    "                                               percentile_slices_to_featurize=[(0., 100.), (90, 100)],\n",
    "                                               summary_ops=[\"std\", \"time_since_measured\", \n",
    "                                                            \"count\", \"slope\", \"median\", \n",
    "                                                            \"min\", \"max\", \"last_value_measured\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "CREATING TRAIN/VALID/TEST SPLITS FOR 1.200 PERCENT OF SEQUENCES LABELLED\n",
      "---------------------------------------------------------------------------\n",
      "Excluded inds train: 15662, 21871, 28805 ... 29562, 34572, 21708\n",
      "Excluded inds valid: 9691, 7743, 866 ... 4256, 6648, 5532\n",
      "Excluded inds test: 12636, 14342, 224 ... 5861, 7136, 14248\n",
      "Saving data to /cluster/tufts/hugheslab/prath01/datasets/eicu_ssl/percentage_labelled_sequnces=1.2\n",
      "Done saving train..\n",
      "Done saving valid..\n",
      "Done saving test..\n"
     ]
    }
   ],
   "source": [
    "state_id = 41\n",
    "data_save_dir = '/cluster/tufts/hugheslab/prath01/datasets/eicu_ssl/'\n",
    "\n",
    "for ii, perc_labelled in enumerate([1.2]):#3.7, 11.1, 33.3, 100\n",
    "    curr_save_dir = os.path.join(data_save_dir, 'percentage_labelled_sequnces=%s'%perc_labelled)\n",
    "    \n",
    "    print('---------------------------------------------------------------------------')\n",
    "    print('CREATING TRAIN/VALID/TEST SPLITS FOR %.3f PERCENT OF SEQUENCES LABELLED'%perc_labelled)\n",
    "    print('---------------------------------------------------------------------------')\n",
    "    y_train_ss = y_train.copy()\n",
    "    rnd_state = np.random.RandomState(state_id)\n",
    "    n_unlabelled_tr = int((1-(perc_labelled)/100)*N_tr)\n",
    "    unlabelled_inds_tr = rnd_state.permutation(N_tr)[:n_unlabelled_tr]\n",
    "    y_train_ss = y_train_ss.astype(np.float32)\n",
    "    y_train_ss[unlabelled_inds_tr] = np.nan  \n",
    "    if perc_labelled!=100:\n",
    "        print('Excluded inds train: %d, %d, %d ... %d, %d, %d'%(unlabelled_inds_tr[0],\n",
    "                                                              unlabelled_inds_tr[1],\n",
    "                                                              unlabelled_inds_tr[2],\n",
    "                                                              unlabelled_inds_tr[-3],\n",
    "                                                              unlabelled_inds_tr[-2],\n",
    "                                                              unlabelled_inds_tr[-1]))\n",
    "    \n",
    "    y_valid_ss = y_valid.copy()\n",
    "    rnd_state = np.random.RandomState(state_id)\n",
    "    n_unlabelled_va = int((1-(perc_labelled)/100)*N_va)\n",
    "    unlabelled_inds_va = rnd_state.permutation(N_va)[:n_unlabelled_va]\n",
    "    y_valid_ss = y_valid_ss.astype(np.float32)\n",
    "    y_valid_ss[unlabelled_inds_va] = np.nan \n",
    "    if perc_labelled!=100:\n",
    "        print('Excluded inds valid: %d, %d, %d ... %d, %d, %d'%(unlabelled_inds_va[0],\n",
    "                                                          unlabelled_inds_va[1],\n",
    "                                                          unlabelled_inds_va[2],\n",
    "                                                          unlabelled_inds_va[-3],\n",
    "                                                          unlabelled_inds_va[-2],\n",
    "                                                          unlabelled_inds_va[-1]))\n",
    "\n",
    "    y_test_ss = y_test.copy()\n",
    "    rnd_state = np.random.RandomState(state_id)\n",
    "    n_unlabelled_te = int((1-(perc_labelled)/100)*N_te)\n",
    "    unlabelled_inds_te = rnd_state.permutation(N_te)[:n_unlabelled_te]\n",
    "    y_test_ss = y_test_ss.astype(np.float32)\n",
    "    y_test_ss[unlabelled_inds_te] = np.nan\n",
    "    if perc_labelled!=100:\n",
    "        print('Excluded inds test: %d, %d, %d ... %d, %d, %d'%(unlabelled_inds_te[0],\n",
    "                                                          unlabelled_inds_te[1],\n",
    "                                                          unlabelled_inds_te[2],\n",
    "                                                          unlabelled_inds_te[-3],\n",
    "                                                          unlabelled_inds_te[-2],\n",
    "                                                          unlabelled_inds_te[-1]))\n",
    "    \n",
    "    # Check whether the specified path exists or not\n",
    "    isExist = os.path.exists(curr_save_dir)\n",
    "\n",
    "    if not isExist:\n",
    "        # Create a new directory because it does not exist \n",
    "        os.makedirs(curr_save_dir)\n",
    "        \n",
    "    # save the data to the respective folder\n",
    "    print('Saving data to %s'%curr_save_dir)\n",
    "    np.save(os.path.join(curr_save_dir, 'X_train_collapsed.npy'), train_x_collapsed_NF)\n",
    "    np.save(os.path.join(curr_save_dir, 'y_train_collapsed.npy'), y_train_ss)\n",
    "    print('Done saving train..')\n",
    "    np.save(os.path.join(curr_save_dir, 'X_valid_collapsed.npy'), valid_x_collapsed_NF)\n",
    "    np.save(os.path.join(curr_save_dir, 'y_valid_collapsed.npy'), y_valid_ss)\n",
    "    print('Done saving valid..')\n",
    "    np.save(os.path.join(curr_save_dir, 'X_test_collapsed.npy'), test_x_collapsed_NF)\n",
    "    np.save(os.path.join(curr_save_dir, 'y_test_collapsed.npy'), y_test_ss)\n",
    "    print('Done saving test..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
